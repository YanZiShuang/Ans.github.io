<html>
  <head>
    <!-- website title-->
    <title>Lily Su's Blog </title>
    <!-- icon-->

    <link rel="icon" type="image/x-icon" href="../images/favicon.ico"/>
    <meta charset="utf-8"/>
    <link rel='stylesheet' type='text/css' href='../css/custom.css'>
  </head>
  <body>
    <h1>Benchmark</h1>
    <div id="toc_container">
      <p class="toc_title">Contents</p>
      <li>
	<a href="#cpu_title">CPU</a>
      </li>
      <li>
	<a href="#memory_title">Memory</a>
      </li>
      <li>
	<a href="#disk_title">Disk</a>
	<ul class="toc_list">
	  <li>
	    <a href="#disk_write_test">3.1 磁盘随机读写测试</a>
	  </li>
	  <li>
	    <a href="#disk_read_test">3.2 磁盘随机读取测试</a>
	  </li>
	</ul>
      </li>
      <ul class="toc_list">
    </div>
    <p>
      记录如何在Linux上面进行性能测试
    </p>
    <h2 id="cpu_title">CPU</h2>
    
    <h2 id="memory_title">Memory</h2>
    <h2 id="disk_title">Disk</h2>
    <p>
      对于服务器磁盘的存储性能测试，大致可以分为顺序读写和随机读写两种，主要关注的结果为带宽（即单位时间内能够传输的数据量）和IOPS（即每秒进行读写操作的次数）。其含义如下
      <li>
	顺序读写（侧重于衡量吞吐量，常用单位为MB/s）：文件在硬盘上的存储位置是连续的。
	适用场景：大文件拷贝，比如说视频，音乐等，虽然数据好看但是对数据库性能也没有什么参考价值。
      </li>
      <li>
	随机读写（侧重于衡量IOPS，常用单位为 次数/s）：按照块大小的不同在硬盘上的随机位置读写数据。
	适用场景：操作系统运行、应用程序运行、数据库等等。
      </li>
      建议直接通过裸盘的方式进行性能测试，这样会得到较为真实的数据。但是直接测试裸盘会破坏文件系统，导致数据丢失。
    </p>
    <h3 id="disk_write_test"> 3.1 硬盘随机读写测试 </h3>
    <p>
      这里我们使用工具<code>dd</code>和<code>fio</code>来进行测试。
      <ol>
	<li>
	  首先需要安装<code>fio</code>和<code>atop</code>工具
	  <pre><code>
yum install -y fio atop
	  </pre></code>
	</li>
	<li>
	  在安装完成之后清除系统缓存
	  <pre><code>
echo 3 > /proc/sys/vm/drops_caches	  
	  </pre></code>
	</li>
	<li>
	  使用<code>fio</code>工具在磁盘上进行随机写性能测试。
	  <pre><code>
for i in `mount |grep mapper |awk '{print $3}'` ;
do (fio -direct=1 -iodepth=128 -rw=randwrite -ioengine=libaio -bs=4k -size=5G -numjobs=1 -runtime=300 -group_reporting -name=$i/testfile);
done
	  </pre></code>
	  这里是以过滤出来的mapper 来找到lvm 的挂载点，以4KB大小的文件在每个mapper挂载点中创建一个大小为5GB的testfile文件。
	</li>
	<li>
	  同时，执行<code>atop</code>命令观察系统负载情况。
	</li>
      </ol>
    </p>

    <h3 id="disk_read_test"> 3.2 磁盘随机读取测试 </h3>
	<p>
	  <ol>
	    <li>
	  在测试之前同样需要清除缓存
	  <pre><code>
echo 3 > /proc/sys/vm/drop_caches	      
	  </pre></code>
	    </li>
	    <li>
	      使用fio工具在磁盘上执行随机读取性能测试
	      <pre><code>
for i in `mount |grep mapper |awk '{print $3}'` ;
do (fio -direct=1 -iodepth=128 -rw=randread -ioengine=libaio -bs=4k -size=5G -numjobs=1 -runtime=300 -group_reporting -name=$i/testfile);
done		  
	      </pre></code>
	    </li>
	  </ol>
	</p>
    <h2>网卡</h2>
    <p>
      评估网络质量可以从如下几个方面考虑
      <ol>
	<li>
	  带宽：网络的吞吐率，用户端到服务端之间可以传输数据的最大速率。不管是在测试还是在正式使用网络服务时，这个指标都是限定的量。带宽当然是越大越好，一般超过限制带宽的<code>50%</code>就会出现严重的丢包情况，这个可以通过，<code>nc</code>、<code>wget</code>、<code>curl</code>、<code>iperf</code>、<code>scp</code>等工具来进行测试。	 
	</li>
	<li>
	  延时：数据包从本地端传输到目的地需要的时间，可以使用<code>ping</code>命令的<code>RTT</code>值来反映其状况。
	</li>
	<li>
	  丢包：这个这表应该适用于任何WEB应用，数据量越大则质量越差，可以通过<code>ping</code>、<code>iperf</code>等工具来查看。
	</li>
	<li>
	  抖动：最好的情况是网络抖动稳定在某个偶去，如果抖动出现比较大的波动，则从一定程度上说明网络质量问题，可以使用 <code>iperf UDP</code>和<code>ping</code>命令的<code>mdev</code>得出比较准确的情况。
	</li>
	
      </ol>
      
    </p>
    <p>
      可以使用<code>ethool</code>命令查看网络情况。
      <pre><code>
ethtool ens33	  
Settings for ens33:
	Supported ports: [ TP ]
	Supported link modes:   10baseT/Half 10baseT/Full
	                        100baseT/Half 100baseT/Full
	                        1000baseT/Full
	Supported pause frame use: No
	Supports auto-negotiation: Yes
	Supported FEC modes: Not reported
	Advertised link modes:  10baseT/Half 10baseT/Full
	                        100baseT/Half 100baseT/Full
	                        1000baseT/Full
	Advertised pause frame use: No
	Advertised auto-negotiation: Yes
	Advertised FEC modes: Not reported
	Speed: 1000Mb/s
	Duplex: Full
	Port: Twisted Pair
	PHYAD: 0
	Transceiver: internal
	Auto-negotiation: on
	MDI-X: off (auto)
	Supports Wake-on: d
	Wake-on: d
	Current message level: 0x00000007 (7)
			       drv probe link
	Link detected: yes	  
      </pre></code>
      这里可以看到输出结果的Speed:1000Mb/s 这里就是千兆网卡。
    </p>
    <h3>iperf</h3>
    <p>
      <code>iperf</code>是一个网络性能测试工具，它可以测试<code>TCP</code>和<code>UDP</code>的带宽质量，以及测试最大<code>TCP</code>带宽，同时可以报告带宽、延迟抖动和数据包丢失等信息。利用这些特性，可以测试一些网络设备例如路由器、防火墙、交换机等的性能。<code>iperf</code>的好处是纯粹发包和接收，防止硬盘影响测试的结果。
      在这里使用的是vmware fusion虚拟机使用的网卡驱动是e1000，系统版本为CentOS Linux release 7.6 为例子进行测试。
      <ol>
	<li>
	  首先在两台节点上分别安装<code>iperf</code>。
	  <pre><code>
yum install -y iperf	      
	  </pre></code>
	</li>
	<li>
	  选择一台服务器以服务端方式运行，并开放防火墙5001端口。
	  <pre><code>
firewall-cmd --zone=public --permanent --add-port=5001/tcp
firewall-cmd --reload
iperf -s -i 1 -w 448k
	  </pre></code>
	</li>
	<li>
	  在另一台服务器上以客户端方式连接到服务端，测试10分钟。
	  <pre><code>
iperf -c 172.16.146.130 -i 1 -w 448k -t 600	      
	  </pre></code>
	  此时服务端会显示如下信息
	  <pre><code>
Server listening on TCP port 5001
TCP window size:  416 KByte (WARNING: requested  438 KByte)
------------------------------------------------------------
[  4] local 172.16.146.130 port 5001 connected with 172.16.146.133 port 53506
[ ID] Interval       Transfer     Bandwidth
[  4]  0.0- 1.0 sec  51.7 MBytes   434 Mbits/sec
[  4]  1.0- 2.0 sec  82.1 MBytes   688 Mbits/sec
[  4]  2.0- 3.0 sec  77.3 MBytes   649 Mbits/sec
[  4]  3.0- 4.0 sec  80.2 MBytes   673 Mbits/sec
[  4]  4.0- 5.0 sec  77.5 MBytes   650 Mbits/sec
[  4]  5.0- 6.0 sec  81.8 MBytes   687 Mbits/sec
[  4]  0.0- 6.1 sec   452 MBytes   620 Mbits/sec	      
	  </pre></code>
	  此时客户端会显示如下信息
	  <pre><code>
Client connecting to 172.16.146.130, TCP port 5001
TCP window size:  416 KByte (WARNING: requested  438 KByte)
------------------------------------------------------------
[  3] local 172.16.146.133 port 53506 connected with 172.16.146.130 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec  51.5 MBytes   432 Mbits/sec
[  3]  1.0- 2.0 sec  81.8 MBytes   686 Mbits/sec
[  3]  2.0- 3.0 sec  77.4 MBytes   649 Mbits/sec
[  3]  3.0- 4.0 sec  80.4 MBytes   674 Mbits/sec
[  3]  4.0- 5.0 sec  77.4 MBytes   649 Mbits/sec
[  3]  5.0- 6.0 sec  81.9 MBytes   687 Mbits/sec	      
	  </pre></code>
	  以上信息显示，当前网卡传输速度约为650Mb/s，不是很接近理论1Gb/s，因为我这个笔记本开两台虚拟机跑测试CPU直接就满负载了。
	  这也就是需要注意的影响到带块的原因，还有许多原因如下：
	  <ol>
	    <li>
	      网络通讯设备，比如说交换机是百兆、或者是千兆之类的。
	    </li>
	    <li>
	      服务器千兆网卡工作在百兆模式下，导致参与测试的任意一台机器工作在百兆模式下都会得到测试不准的数据。
	    </li>
	    <li>
	      使用真是的磁盘文件进行测试，磁盘的速度会影响到网络的传输速率。
	    </li>
	    <li>
	      服务器和交换机连接的线缆，比如说光纤和传统的双绞线等。
	    </li>
	  </ol>
	  此外，如果服务器由多个网口，则需要将这些网口配置成不同的网段，数据传输时走杜立德网口，避免同一网段的多对网口数据传输走同一路由，导致速率下降，测试不准。
	</li>
      </ol>
    </p>
    <h3> netperf</h3>
    <p>
      <code>netperf</code>也是一个网络性能测试工具，主要是基于<code>TCP</code>或者是<code>UDP</code>传输。该工具以客户端/服务端方式工作。服务器是<code>netserver</code>，用来侦听来自客户端的连接；客户端是<code>netperf</code>，用来向服务端发起网络测试。
      <code>netperf</code>常用命令参数如下：
      <ul>
      <li>
	-H host 指定远端运行<code>netserver</code>的服务器地址。
      </li>
      <li>
	-l testlen 只听测试的时间长度（秒）。
      </li>
      <li>
	-t testname 指定测试类型，包括TCP_STREAM、UPD_STREAM、TCP_RR、TCP_CRR、UDP_RR。
      </ul>
    </p>
    </ol>
    <p>
      在这里测试环境为在Vmware Fusion中运行的CentOS Linux release 7.6虚拟机两台，网络为1000Mb/s 网络，服务器端和客户端通过同一个虚拟网络进行连接。
      <ol>
	<li>
	  在服务端和客户端安装<code>netperf</code>
	  <pre><code>
wget -c https://github.com/HewlettPackard/netperf/archive/netperf-2.7.0.tar.gz
cd netperf-netperf-2.7.0/
./configure
make && make install
	  </pre></code>
	</li>
	<li>
	  TCP_STREAM
	  在默认情况下，<code>netperf</code>进行<code>TCP</code>数据批量传输，即<code>-t TCP_STREAM</code>。在测试过程中，<code>netperf</code>向<code>netserver</code>发送批量的<code>TCP</code>数据分组，以确定数据传输过程中的吞吐量。
	  <p>
	    启动服务端
	  </p>
	  <pre><code>
firewall-cmd --zone=public --permanent --add-port=12865/tcp
firewall-cmd --reload	      
netserver -D	      
	  </pre></code>
	  <p>
	    启动客户端
	  </p>
	    <pre><code>
netperf -H 172.16.146.130 -l 60		
	    </pre></code>

</li>
从测试结果我们可以知道以下信息。
  <pre><code>
MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 172.16.146.130 () port 0 AF_INET
Recv   Send    Send
Socket Socket  Message  Elapsed
Size   Size    Size     Time     Throughput
bytes  bytes   bytes    secs.    10^6bits/sec

 87380  16384  16384    60.00     953.89      
  </pre></code>
  <ul>
    <li>
      服务端使用大小为87380字节的Socket接收缓冲区。
    </li>
    <li>
      客户端使用大小为16384字节的Socket发送缓冲区。
    </li>
    <li>
      向远端系统发送的测速组大小为16384字节。
    </li>
    <li>
      测试的用时为60秒
    </li>
    <li>
      最后的测试结果为953.89Mb/s
    </li>
  </ul>
  
  <li>
    UDP_STREAM
    <p>
      UDP_STREAM用来测试UDP用户数据报协议批量传输时的网络性能。需要注意的是，此时测试分组的大小不得大于Socket的发送与接收缓冲区大小，否则<code>netperf</code>会出现报错提示。
      <pre><code>
MIGRATED UDP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 172.16.146.130 () port 0 AF_INET
Socket  Message  Elapsed      Messages
Size    Size     Time         Okay Errors   Throughput
bytes   bytes    secs            #      #   10^6bits/sec

212992   65507   60.00       32795      0     286.43
212992           60.00       32743            285.98
	  
      </pre></code>
    </p>
  </li>
</ol>
</p>

    <big>
      <a href="../index.html">Return HomePage</a>
    </big>
  </body>
  
</html>
